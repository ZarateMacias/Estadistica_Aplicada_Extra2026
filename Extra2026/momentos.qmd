---
title: "Estimación por el Método de Momentos"
lang: es
format:
  html:
    toc: false
    theme: cosmo
    code-fold: true
    fig-width: 6
    fig-height: 4
    fontsize: 1.1rem
---

```{=html}
<style>
main.content {
text-align: justify}
</style>
```

```{r}
#| include: false

library(tidyverse)
library(knitr)
library(kableExtra)
library(readxl)
library(RColorBrewer)
```

Suponiendo dada una muestra aleatoria de tamaño $n$ para cada una de las siguientes distribuciones realiza lo siguiente:

a)  Encuentra el estimador para $\theta$ por el método de momentos.

b)  Verifica si es insesgado y/o asintóticamente insesgado.

    En este inciso será de utilidad recordar la esperanza de la media muestral:

$$E(\overline{X}) = E\left(\frac{1}{n}\sum_{i=1}^n X_i\right) = \frac{1}{n}\sum_{i=1}^n E(X_i) = \frac{1}{n} n E(X) = E(X).$$

c)  Calcula la varianza del estimador.

    Es conveniente recordar algunas propiedades de la varianza que se enuncian en la siguiente proposición:

::: {#prp-varianza}
Sean $X$ y $Y$ dos variables aleatorias con varianza finita y sea $c$ una constante, entonces:

1.  $Var(c)=0$.
2.  $Var(cX) = c^2 Var(X)$.
3.  $Var(X+c) = Var(X)$.
4.  $Var(X) = E(X^2) - [E(X)]^2$.
5.  $Var(X+Y) = Var(X) + Var(Y) + 2Cov(X,Y)$, donde $$Cov(X,Y) = E[(X - E(X))(Y - E(Y))]$$ es la covarianza entre $X$ y $Y$. Si $X$ y $Y$ son independientes, entonces $Cov(X,Y) = 0$ y por lo tanto $Var(X+Y) = Var(X) + Var(Y)$.
:::

Además, dado que en una muestra aleatoria consideramos variables aleatorias independientes:

$$Var{\overline{X}} = Var\left(\frac{1}{n}\sum_{i=1}^n X_i\right) = \frac{1}{n^2}Var\left(\sum_{i=1}^n X_i\right) = \frac{1}{n^2}\sum_{i=1}^n Var(X_i) = \frac{1}{n^2} n Var(X) = \frac{Var(X)}{n}.$$

d)  Calcula el error cuadrático medio (ECM).

e)  Elige un valor para $\theta$ y simula una muestra aleatoria de tamaño $n=1000$. Calcula el estimador, para los ejercicios 1 y 2 (distribuciones discretas): genera una muestra aleatoria de tamaño $n=1000$ utilizando el valor estimado del parámetro y compara ambos histogramas. Para los ejercicios 3 y 4 (distribuciones continuas): compara el histograma de los valores simulados con el valor real del parámetro y la función de densidad obtenida con el valor estimado del parámetro.

f)  Verifica la convergencia del estimador al aumentar el tamaño cada muestra. Grafica los valores del estimador en función del tamaño de la muestra (puede ser por medio de un boxplot).

:::: {#exm-discreta}
Para $0 < \theta < 6/5$, consideramos la función de probabilidad:

\begin{equation}
f(x;\theta) = \begin{cases}
\theta/2 & \text{si } x = -1, \\
\theta/3 & \text{si } x = 0, \\
1-5\theta/6 & \text{si } x = 1, \\
0 & \text{en otro caso.}
\end{cases}
\end{equation}

::: panel-tabset
## a) Estimador

Encontramos el estimador para $\theta$ por el método de momentos.

Primero calculamos la esperanza de la variable aleatoria $X$:

\begin{equation}
E(X) = \sum_x x f(x;\theta) = (-1)(\theta
/2) + (0)(\theta/3) + (1)(1-5\theta/6) = 1 - \frac{4\theta}{3}
\end{equation}

Luego, igualamos la esperanza muestral a la esperanza teórica para encontrar el estimador por el método de momentos:

\begin{equation}
\overline{X} = 1 - \frac{4\hat\theta}{3} \implies \hat\theta = \frac{3(1 - \bar{X})}{4}
\end{equation}

## b) Insesgamiento

Verificamos si es insesgado y/o asintóticamente insesgado.

\begin{equation}
E(\hat\theta) = E\left(\frac{3(1 - \overline{X})}{4}\right) = \frac{3}{4}(1 - E(\overline{X})) = \frac{3}{4}(1 - E(X)) = \frac{3}{4}\left(1 - \left(1 - \frac{4\theta}{3}\right)\right) = \theta
\end{equation}

Luego, el estimador es insesgado y por lo tanto, asintóticamente insesgado.

## c) Varianza

Calculamos la varianza del estimador.

\begin{equation}
Var(\hat\theta) = Var\left(\frac{3(1 - \overline{X})}{4}\right) = \left(\frac{3}{4}\right)^2 Var(1 - \overline{X}) = \left(\frac{3}{4}\right)^2 Var(\overline{X}) = \left(\frac{3}{4}\right)^2 \frac{Var(X)}{n}
\end{equation}

Para calcular $Var(X)$ utilizamos la igualdad $Var(X) = E(X^2) - [E(X)]^2$ y para ello, inicialmente calculamos $E(X^2)$:

\begin{equation}
E(X^2) = \sum_x x^2 f(x;\theta) = (-1)^2(\theta/2) + (0)^2(\theta/3) + (1)^2(1 - 5\theta/6) = \frac{\theta}{2} + 1 - \frac{5\theta}{6} = 1 - \frac{\theta}{3}
\end{equation}

Luego, calculamos la varianza de $X$:

\begin{equation}
Var(X) = E(X^2) - [E(X)]^2 = \left(1 - \frac{\theta}{3}\right) - \left(1 - \frac{4\theta}{3}\right)^2 = \frac{-\theta^2 + 6\theta}{9}
\end{equation}

Finalmente, sustituimos $Var(X)$ en la expresión de $Var(\hat\theta)$:

\begin{equation}
Var(\hat\theta) = \left(\frac{3}{4}\right)^2 \frac{1}{n} \frac{-\theta^2 + 6\theta}{9} = \frac{-\theta^2 + 6\theta}{16n}
\end{equation}

## d) ECM

Calculamos el error cuadrático medio (ECM).

Dado que el estimador es insesgado, el ECM es igual a la varianza:

\begin{equation}
ECM(\hat\theta) = Var(\hat\theta) = \frac{-\theta^2 + 6\theta}{16n}
\end{equation}

## e) Simulación

Elegimos el valor $\theta=1$ y simulamos una muestra aleatoria de tamaño $n=1000$. Calculamos el estimador, graficamos el histograma de los datos y lo comparamos con la función de probabilidad obtenida con el parámetro estimado.

```{r}
# Parámetro fijo
theta_fijo <- 1

# Función de probabilidad 
dexr <- function(x, theta){
f_x <-ifelse(x == -1, theta/2, ifelse(x == 0, theta/3, ifelse(x == 1, 1 - 5*theta/6, 0)))
return(f_x)
}

# Función para generar muestra aleatoria
rexr <- function(n, theta){
  X <- sample(c(-1, 0, 1), size = n, replace = TRUE, prob = c(theta/2, theta/3, 1 - 5*theta/6))
  return(X)
}

# Función para estimar theta
estimar_theta <- function(X){
  theta_hat <- (3 * (1 - mean(X))) / 4
  return(theta_hat)
}


df_exr <- data.frame(X = rexr(1000, theta_fijo), Tipo = "Teórico")

theta_hat <- estimar_theta(df_exr$X)

df_temp <- data.frame(X = rexr(1000, theta_hat), Tipo = "Estimados")

df_exr <- rbind(df_exr, df_temp)

ggplot(df_exr)+
  geom_histogram(aes(x = X, y = after_stat(density), fill =Tipo), bins = 3, center = -1, color = "black", alpha = 0.7, position ="dodge")+
  scale_fill_brewer(palette = "Set1")+
  theme_bw()
  





```

## f) Convergencia

Verificamos la convergencia del estimador al aumentar el tamaño cada muestra. Graficamos los valores del estimador en función del tamaño de la muestra para $n = 10, 50, 100, 500, 1000$. Para cada $n$ se generan $N = 500$ valores.

```{r}
# Tamaños de muestra y número de réplicas
tamano <- c(10, 50, 100, 500, 1000)
N <- 500

# Data frame para almacenar los resultados

df_convergencia <- data.frame()

# Simulación y cálculo del estimador para cada tamaño de muestra
for (n in tamano){
  estimacion_n <- replicate(N, {
    X <- rexr(n, theta_fijo)
    estimar_theta(X)
  })
  df_temp <- data.frame(n = as.factor(n), Estimacion = estimacion_n)
  df_convergencia <- rbind(df_convergencia, df_temp)
}

# Gráfico de convergencia

ggplot(df_convergencia)+
  geom_boxplot(aes(x = n, y = Estimacion, fill = n), alpha = 0.7)+
  geom_hline(yintercept = theta_fijo, color = "red", linetype = "dashed", linewidth = 1)+
  scale_fill_brewer(palette = "Set1")+
  labs(title = "Convergencia del estimador al aumentar el tamaño de la muestra",
       x = "Tamaño de la muestra (n)",
       y = "Estimación de θ")+
  theme_bw()+
  theme(legend.position = "none")

```
:::
::::

::: {#exr-discreta_1}
Para $0 < \theta < 4$, consideramos la función de probabilidad:

\begin{equation}
f(x;\theta) = \begin{cases}
\theta/4 & \text{si } x = 1, \\
1 - \theta/4 & \text{si } x = 2, \\
0 & \text{en otro caso.}
\end{cases}
\end{equation}
:::

::: panel-tabset
## a) Estimador

Encontramos el estimador para $\theta$ por el método de momentos.

Primero calculamos la esperanza de la variable aleatoria $X$:

$$E(X) = \sum_x x f(x;\theta) = (1)(\theta/4) + (2)(1-\theta/4) = \frac{\theta}{4} + 2 - \frac{2\theta}{4} = 2 - \frac{\theta}{4}$$

Luego, igualamos la esperanza muestral a la esperanza teórica para encontrar el estimador por el método de momentos:

$$\overline{X} = 2 - \frac{\hat\theta}{4} \implies \hat\theta = 8 - 4\overline{X}$$

## b) Insesgamiento

Verificamos si es insesgado y/o asintóticamente insesgado.

$$E(\hat\theta) = E(8 - 4\overline{X}) = 8 - 4E(\overline{X}) = 8 - 4E(X) = 8 - 4\left(2 - \frac{\theta}{4}\right) = 8 - 8 + \theta = \theta$$

Luego, el estimador es insesgado y por lo tanto, asintóticamente insesgado.

## c) Varianza

Calculamos la varianza del estimador.

$$Var(\hat\theta) = Var(8 - 4\overline{X}) = Var(-4\overline{X}) = 16 Var(\overline{X}) = 16 \frac{Var(X)}{n}$$

Para calcular $Var(X)$ utilizamos la igualdad $Var(X) = E(X^2) - [E(X)]^2$ y para ello, inicialmente calculamos $E(X^2)$:

$$E(X^2) = \sum_x x^2 f(x;\theta) = (1)^2(\theta/4) + (2)^2(1 - \theta/4) = \frac{\theta}{4} + 4 - \theta = 4 - \frac{3\theta}{4}$$

Luego, calculamos la varianza de $X$:

$$Var(X) = E(X^2) - [E(X)]^2 = \left(4 - \frac{3\theta}{4}\right) - \left(2 - \frac{\theta}{4}\right)^2$$

$$= 4 - \frac{3\theta}{4} - \left(4 - \theta + \frac{\theta^2}{16}\right) = 4 - \frac{3\theta}{4} - 4 + \theta - \frac{\theta^2}{16}$$

$$= \frac{\theta}{4} - \frac{\theta^2}{16} = \frac{4\theta - \theta^2}{16}$$

Finalmente, sustituimos $Var(X)$ en la expresión de $Var(\hat\theta)$:

$$Var(\hat\theta) = 16 \cdot \frac{1}{n} \cdot \frac{4\theta - \theta^2}{16} = \frac{4\theta - \theta^2}{n}$$

## d) ECM

Calculamos el error cuadrático medio (ECM).

Dado que el estimador es insesgado, el ECM es igual a la varianza:

$$ECM(\hat\theta) = Var(\hat\theta) = \frac{4\theta - \theta^2}{n}$$

## e) Simulación

Elegimos el valor $\theta=2$ y simulamos una muestra aleatoria de tamaño $n=1000$. Calculamos el estimador, generamos una muestra aleatoria de tamaño $n=1000$ utilizando el valor estimado del parámetro y comparamos ambos histogramas.

```{r}
# Parámetro fijo
theta_fijo <- 2

# Función de probabilidad 
dexr <- function(x, theta){
  f_x <- ifelse(x == 1, theta/4, ifelse(x == 2, 1 - theta/4, 0))
  return(f_x)
}

# Función para generar muestra aleatoria
rexr <- function(n, theta){
  X <- sample(c(1, 2), size = n, replace = TRUE, 
              prob = c(theta/4, 1 - theta/4))
  return(X)
}

# Función para estimar theta
estimar_theta <- function(X){
  theta_hat <- 8 - 4 * mean(X)
  return(theta_hat)
}

# Generar muestra con theta fijo
df_exr <- data.frame(X = rexr(1000, theta_fijo), Tipo = "Teórico")

# Estimar theta
theta_hat <- estimar_theta(df_exr$X)

# Generar muestra con theta estimado
df_temp <- data.frame(X = rexr(1000, theta_hat), Tipo = "Estimado")

# Combinar ambas muestras
df_exr <- rbind(df_exr, df_temp)

# Gráfico
library(ggplot2)
library(RColorBrewer)

ggplot(df_exr) +
  geom_histogram(aes(x = X, y = after_stat(density), fill = Tipo), 
                 bins = 2, center = 1.5, color = "black", 
                 alpha = 0.7, position = "dodge") +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Comparación de distribuciones",
       x = "Valores",
       y = "Densidad") +
  theme_bw()
```

**Resultado:** Con $\theta = 2$, el valor estimado es $\hat\theta \approx 2.00$ (varía en cada simulación).

## f) Convergencia

Verificamos la convergencia del estimador al aumentar el tamaño cada muestra. Graficamos los valores del estimador en función del tamaño de la muestra para $n = 10, 50, 100, 500, 1000$. Para cada $n$ se generan $N = 500$ valores.

```{r}
# Tamaños de muestra y número de réplicas
tamano <- c(10, 50, 100, 500, 1000)
N <- 500

# Data frame para almacenar los resultados
df_convergencia <- data.frame()

# Simulación y cálculo del estimador para cada tamaño de muestra
for (n in tamano){
  estimacion_n <- replicate(N, {
    X <- rexr(n, theta_fijo)
    estimar_theta(X)
  })
  df_temp <- data.frame(n = as.factor(n), Estimacion = estimacion_n)
  df_convergencia <- rbind(df_convergencia, df_temp)
}

# Gráfico de convergencia
ggplot(df_convergencia) +
  geom_boxplot(aes(x = n, y = Estimacion, fill = n), alpha = 0.7) +
  geom_hline(yintercept = theta_fijo, color = "red", 
             linetype = "dashed", linewidth = 1) +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Convergencia del estimador al aumentar el tamaño de la muestra",
       x = "Tamaño de la muestra (n)",
       y = "Estimación de θ") +
  theme_bw() +
  theme(legend.position = "none")
```

El gráfico muestra que a medida que aumenta el tamaño de la muestra, las estimaciones se concentran cada vez más alrededor del valor real $\theta = 2$ (línea roja punteada), confirmando la **consistencia** del estimador.
:::
::::

::: {#exr-discreta_2}
Para $0 < \theta < 3/2$, consideramos la función de probabilidad:

\begin{equation}
f(x;\theta) = \begin{cases}
\theta/3 & \text{si } x = 0, \\
1-2\theta/3 & \text{si } x = 1, \\
\theta/3 & \text{si } x = 2, \\
0 & \text{en otro caso.}
\end{cases}
\end{equation}
:::

::: panel-tabset
## a) Estimador

Encontramos el estimador para $\theta$ por el método de momentos.

Primero calculamos la esperanza de la variable aleatoria $X$:

$$E(X) = \sum_x x f(x;\theta) = (0)(\theta/3) + (1)(1-2\theta/3) + (2)(\theta/3) = 0 + 1 - \frac{2\theta}{3} + \frac{2\theta}{3} = 1$$

Observamos que $E(X) = 1$ es constante y no depende de $\theta$, por lo que **no podemos usar el primer momento** para estimar $\theta$.

Utilizamos el segundo momento. Calculamos $E(X^2)$:

$$E(X^2) = \sum_x x^2 f(x;\theta) = (0)^2(\theta/3) + (1)^2(1-2\theta/3) + (2)^2(\theta/3)$$

$$= 0 + 1 - \frac{2\theta}{3} + \frac{4\theta}{3} = 1 + \frac{2\theta}{3}$$

Igualamos el segundo momento muestral con el segundo momento poblacional:

$$\frac{1}{n}\sum_{i=1}^n X_i^2 = 1 + \frac{2\hat\theta}{3}$$

Despejando $\hat\theta$:

$$\hat\theta = \frac{3}{2}\left(\frac{1}{n}\sum_{i=1}^n X_i^2 - 1\right)$$

## b) Insesgamiento

Verificamos si es insesgado y/o asintóticamente insesgado.

$$E(\hat\theta) = E\left[\frac{3}{2}\left(\frac{1}{n}\sum_{i=1}^n X_i^2 - 1\right)\right] = \frac{3}{2}\left[E\left(\frac{1}{n}\sum_{i=1}^n X_i^2\right) - 1\right]$$

$$= \frac{3}{2}\left[\frac{1}{n}\sum_{i=1}^n E(X_i^2) - 1\right] = \frac{3}{2}\left[E(X^2) - 1\right]$$

$$= \frac{3}{2}\left[\left(1 + \frac{2\theta}{3}\right) - 1\right] = \frac{3}{2} \cdot \frac{2\theta}{3} = \theta$$

Luego, el estimador es insesgado y por lo tanto, asintóticamente insesgado.

## c) Varianza

Calculamos la varianza del estimador.

$$Var(\hat\theta) = Var\left[\frac{3}{2}\left(\frac{1}{n}\sum_{i=1}^n X_i^2 - 1\right)\right] = \left(\frac{3}{2}\right)^2 Var\left(\frac{1}{n}\sum_{i=1}^n X_i^2\right)$$

$$= \frac{9}{4} \cdot \frac{1}{n^2} Var\left(\sum_{i=1}^n X_i^2\right) = \frac{9}{4n^2} \sum_{i=1}^n Var(X_i^2) = \frac{9}{4n} Var(X^2)$$

Para calcular $Var(X^2)$ necesitamos $E(X^4)$:

$$E(X^4) = \sum_x x^4 f(x;\theta) = (0)^4(\theta/3) + (1)^4(1-2\theta/3) + (2)^4(\theta/3)$$

$$= 0 + 1 - \frac{2\theta}{3} + \frac{16\theta}{3} = 1 + \frac{14\theta}{3}$$

Ahora calculamos $Var(X^2)$:

$$Var(X^2) = E(X^4) - [E(X^2)]^2 = \left(1 + \frac{14\theta}{3}\right) - \left(1 + \frac{2\theta}{3}\right)^2$$

$$= 1 + \frac{14\theta}{3} - \left(1 + \frac{4\theta}{3} + \frac{4\theta^2}{9}\right)$$

$$= 1 + \frac{14\theta}{3} - 1 - \frac{4\theta}{3} - \frac{4\theta^2}{9} = \frac{10\theta}{3} - \frac{4\theta^2}{9}$$

Finalmente, sustituimos en la expresión de $Var(\hat\theta)$:

$$Var(\hat\theta) = \frac{9}{4n}\left(\frac{10\theta}{3} - \frac{4\theta^2}{9}\right) = \frac{9}{4n}\left(\frac{30\theta - 4\theta^2}{9}\right) = \frac{30\theta - 4\theta^2}{4n} = \frac{15\theta - 2\theta^2}{2n}$$

## d) ECM

Calculamos el error cuadrático medio (ECM).

Dado que el estimador es insesgado, el ECM es igual a la varianza:

$$ECM(\hat\theta) = Var(\hat\theta) = \frac{15\theta - 2\theta^2}{2n}$$

## e) Simulación

Elegimos el valor $\theta=1$ y simulamos una muestra aleatoria de tamaño $n=1000$. Calculamos el estimador, generamos una muestra aleatoria de tamaño $n=1000$ utilizando el valor estimado del parámetro y comparamos ambos histogramas.

```{r}
# Parámetro fijo
theta_fijo <- 1

# Función de probabilidad 
dexr <- function(x, theta){
  f_x <- ifelse(x == 0, theta/3, 
                ifelse(x == 1, 1 - 2*theta/3, 
                       ifelse(x == 2, theta/3, 0)))
  return(f_x)
}

# Función para generar muestra aleatoria
rexr <- function(n, theta){
  X <- sample(c(0, 1, 2), size = n, replace = TRUE, 
              prob = c(theta/3, 1 - 2*theta/3, theta/3))
  return(X)
}

# Función para estimar theta
estimar_theta <- function(X){
  theta_hat <- (3/2) * (mean(X^2) - 1)
  return(theta_hat)
}

# Generar muestra con theta fijo
df_exr <- data.frame(X = rexr(1000, theta_fijo), Tipo = "Teórico")

# Estimar theta
theta_hat <- estimar_theta(df_exr$X)

# Generar muestra con theta estimado
df_temp <- data.frame(X = rexr(1000, theta_hat), Tipo = "Estimado")

# Combinar ambas muestras
df_exr <- rbind(df_exr, df_temp)

# Gráfico
library(ggplot2)
library(RColorBrewer)

ggplot(df_exr) +
  geom_histogram(aes(x = X, y = after_stat(density), fill = Tipo), 
                 bins = 3, center = 1, color = "black", 
                 alpha = 0.7, position = "dodge") +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Comparación de distribuciones",
       x = "Valores",
       y = "Densidad") +
  theme_bw()
```

**Resultado:** Con $\theta = 1$, el valor estimado es $\hat\theta \approx 1.00$ (varía en cada simulación).

## f) Convergencia

Verificamos la convergencia del estimador al aumentar el tamaño cada muestra. Graficamos los valores del estimador en función del tamaño de la muestra para $n = 10, 50, 100, 500, 1000$. Para cada $n$ se generan $N = 500$ valores.

```{r}
# Tamaños de muestra y número de réplicas
tamano <- c(10, 50, 100, 500, 1000)
N <- 500

# Data frame para almacenar los resultados
df_convergencia <- data.frame()

# Simulación y cálculo del estimador para cada tamaño de muestra
for (n in tamano){
  estimacion_n <- replicate(N, {
    X <- rexr(n, theta_fijo)
    estimar_theta(X)
  })
  df_temp <- data.frame(n = as.factor(n), Estimacion = estimacion_n)
  df_convergencia <- rbind(df_convergencia, df_temp)
}

# Gráfico de convergencia
ggplot(df_convergencia) +
  geom_boxplot(aes(x = n, y = Estimacion, fill = n), alpha = 0.7) +
  geom_hline(yintercept = theta_fijo, color = "red", 
             linetype = "dashed", linewidth = 1) +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Convergencia del estimador al aumentar el tamaño de la muestra",
       x = "Tamaño de la muestra (n)",
       y = "Estimación de θ") +
  theme_bw() +
  theme(legend.position = "none")
```

El gráfico muestra que a medida que aumenta el tamaño de la muestra, las estimaciones se concentran cada vez más alrededor del valor real $\theta = 1$ (línea roja punteada), confirmando la **consistencia** del estimador.
:::
::::


Para poder generar las muestras aleatorias de las distribuciones continuas de los siguientes ejercicios, es necesario enunciar el siguiente teorema:

::: {#thm-inversion}
Si $X$ es una variable aleatoria continua con función de distribución acumulada $F_X(x)$, entonces la variable aleatoria $U = F_X(X)$ tiene una distribución uniforme en el intervalo $(0,1)$. Además, si $U \sim unif(0,1)$, entonces la variable aleatoria $X = F_X^{-1}(U)$ tiene la misma distribución que $X$.
:::

Para poder aplicar el teorema de inversión, es necesario encontrar la función de distribución acumulada y su inversa. Para que esto último sea posible, es necesario que la función de distribución acumulada sea estrictamente creciente.
::::
:::::


:::: {#exm-continua}
Para $\theta >0$, consideramos la función de densidad:

\begin{equation}
f(x;\theta) = \begin{cases}
\frac{2x}{\theta^2} & \text{si } 0\leq x \leq \theta \\
0 & \text{en otro caso.}
\end{cases}
\end{equation}

::: panel-tabset
## a) Estimador

Encontramos el estimador para $\theta$ por el método de momentos.

Primero calculamos la esperanza de la variable aleatoria $X$:

\begin{eqnarray}
E(X) & = & \int_{-\infty}^{\infty} x f(x;\theta) dx \\
     & = & \int_0^{\theta} x \frac{2x}{\theta^2} dx \\
     & = & \frac{2}{\theta^2} \int_0^{\theta} x^2 dx \\
     & = & \frac{2}{\theta^2} \left[\frac{x^3}{3}\right]_0^{\theta} \\
     & = & \frac{2\theta}{3}
\end{eqnarray}

Ahora igualamos la esperanza muestral a la esperanza teórica para encontrar el estimador por el método de momentos:

\begin{equation}
\overline{X} = \frac{2\hat\theta}{3} \implies \hat\theta = \frac{3\overline{X}}{2}
\end{equation}

## b) Insesgamiento

Verificamos si es insesgado y/o asintóticamente insesgado.

\begin{equation}
E(\hat\theta) = E\left(\frac{3\overline{X}}{2}\right) = \frac{3}{2}E(\overline{X}) = \frac{3}{2}E(X) = \frac{3}{2}\left(\frac{2\theta}{3}\right) = \theta
\end{equation}

Entonces, el estimador es insesgado y por lo tanto, asintóticamente insesgado.

## c) Varianza

Calculamos la varianza del estimador.

\begin{equation}
Var(\hat\theta) = Var\left(\frac{3\overline{X}}{2}\right) = \left(\frac{3}{2}\right)^2 Var(\overline{X}) = \left(\frac{3}{2}\right)^2 \frac{Var(X)}{n}
\end{equation}

Para calcular $Var(X)$ utilizamos la igualdad $Var(X) = E(X^2) - [E(X)]^2$ y para ello, inicialmente calculamos $E(X^2)$:

\begin{eqnarray}
E(X^2) & = & \int_{-\infty}^{\infty} x^2 f(x;\theta) dx \\
       & = & \int_0^{\theta} x^2 \frac{2x}{\theta^2} dx \\
       & = & \frac{2}{\theta^2} \int_0^{\theta} x^3 dx \\
       & = & \frac{2}{\theta^2} \left[\frac{x^4}{4}\right]_0^{\theta} \\
       & = & \frac{\theta^2}{2}
\end{eqnarray}

Luego, calculamos la varianza de $X$:

\begin{equation}
Var(X) = E(X^2) - [E(X)]^2 = \frac{\theta^2}{2} - \left(\frac{2\theta}{3}\right)^2 = \frac{\theta^2}{18}
\end{equation}

Finalmente, sustituimos $Var(X)$ en la expresión de $Var(\hat\theta)$:

\begin{equation}
Var(\hat\theta) = \left(\frac{3}{2}\right)^2 \frac{1}{n}\frac{\theta^2}{18} = \frac{\theta^2}{8n}
\end{equation}

## d) ECM

Calculamos el error cuadrático medio (ECM).

Dado que el estimador es insesgado, el ECM es igual a la varianza:

\begin{equation}
ECM(\hat\theta) = Var(\hat\theta) = \frac{\theta^2}{8n}
\end{equation}

## e) Simulación

Se elige un valor de $\theta = 5$ y se simula una muestra aleatoria de tamaño $n=1000$. Calculamos el estimador, graficamos el histograma de los datos y lo comparamos con la función de densidad obtenida con el parámetro estimado.

En este caso hay que calcular la función de distribución (CDF o probabilidad acumulada)

\begin{eqnarray}
F_X(x;\theta) & = & \int_{-\infty}^{x} f(t;\theta) dt \\
              & = & \int_0^{x} \frac{2t}{\theta^2} dt \\
              & = & \frac{2}{\theta^2} \left[\frac{t^2}{2}\right]_0^{x} \\
              & = & \frac{x^2}{\theta^2}, \quad 0 \leq x \leq \theta
\end{eqnarray}

Observamos que $F_X(x;\theta)$ es estrictamente creciente en el intervalo $(0, \theta)$ y por lo tanto, podemos encontrar su inversa, igualando $U=F_X(x,\theta)$ donde $U \sim unif(0,1)$, tenemos

\begin{equation}
U = \frac{x^2}{\theta^2} \implies x = \theta \sqrt{U}
\end{equation}

Luego, la variable aleatoria $X=F_X^{-1}(U)=\theta \sqrt{U}$ tiene la misma distribución que $X$.

```{r}
#| fig-align: center

# Parámetro fijo

theta_fijo <- 5

# Función de densidad
dexr <- function(x, theta){
  f_x <- ifelse(x >= 0 & x <= theta, (2*x)/(theta^2), 0)
  return(f_x)
}

# Función para generar muestra aleatoria
rexr <- function(n, theta){
  U <- runif(n)
  X <- theta * sqrt(U)
  return(X)
}


# Función para estimar theta

estimar_theta <- function(X){
  theta_hat <- (3 * mean(X)) / 2
  return(theta_hat)
}


df_exr <- data.frame(X = rexr(5000, theta_fijo))
theta_hat <- estimar_theta(df_exr$X)


ggplot(df_exr)+
  geom_histogram(aes(x = X, y = after_stat(density)), binwidth =0.25, color = "black", fill = "coral3", alpha = 0.7, boundary=0)+
  stat_function(fun = dexr, args = list(theta = theta_hat), color = "blue", linewidth = 1, xlim = c(0, theta_hat))+
  annotate("text", x = 4, y = 0.15, label = paste("θ =", round(theta_hat,3)), color = "blue", size = 5)+
  labs(title = "Histograma de datos y función de densidad estimada",
       x = "Valores",
       y = "Densidad")+
  theme_minimal()





```

## f) Convergencia

Verificamos la convergencia del estimador al aumentar el tamaño cada muestra. Graficamos los valores del estimador en función del tamaño de la muestra con $n= 10, 50, 100, 500, 100$. Para cada $n$ se generan $N=500$ valores

```{r}
#| fig-align: "center"

# Tamaños de muestra y número de réplicas
tamano <- c(10, 50, 100, 500, 1000)
N <- 500

# Data frame para almacenar los resultados
df_convergencia <- data.frame()

# Simulación y cálculo del estimador para cada tamaño de muestra
for (n in tamano){
  estimacion_n <- replicate(N, {
    X <- rexr(n, theta_fijo)
    estimar_theta(X)
  })
  df_temp <- data.frame(n = factor(n), Estimacion = estimacion_n )
  df_convergencia <- rbind(df_convergencia, df_temp)
}

# Gráfico de convergencia
ggplot(df_convergencia)+
  geom_boxplot(aes(x = n, y = Estimacion, fill = n), alpha = 0.7)+
  geom_hline(yintercept = theta_fijo, color = "red", linetype = "dashed", linewidth = 1)+
  scale_fill_brewer(palette = "Set1")+
  labs(title = "Convergencia del estimador al aumentar el tamaño de la muestra",
       x = "Tamaño de la muestra (n)",
       y = "Estimación de θ")+
  theme_bw()+
  theme(legend.position = "none")

```
:::

<div>

Para $\theta \in \mathbb{R}$, consideramos la función de densidad:

\begin{equation}
f(x;\theta) = \begin{cases}
e^{-(x-\theta)} & \text{si } \theta \leq x < \infty \\
0 & \text{en otro caso.}
\end{cases}
\end{equation}

</div>
::::

::: panel-tabset

## a) Estimador

Encontramos el estimador para $\theta$ por el método de momentos.

Primero calculamos la esperanza de la variable aleatoria $X$. Esta es una distribución exponencial trasladada.

$$E(X) = \int_{-\infty}^{\infty} x f(x;\theta) dx = \int_{\theta}^{\infty} x e^{-(x-\theta)} dx$$

Realizamos el cambio de variable $u = x - \theta$, entonces $x = u + \theta$ y $dx = du$. Cuando $x = \theta$, $u = 0$:

$$E(X) = \int_{0}^{\infty} (u + \theta) e^{-u} du = \int_{0}^{\infty} u e^{-u} du + \theta \int_{0}^{\infty} e^{-u} du$$

La primera integral es $\Gamma(2) = 1! = 1$ y la segunda integral es $1$:

$$E(X) = 1 + \theta$$

Igualamos con la media muestral:

$$\overline{X} = 1 + \hat\theta \implies \hat\theta = \overline{X} - 1$$

## b) Insesgamiento

Verificamos si es insesgado y/o asintóticamente insesgado.

$$E(\hat\theta) = E(\overline{X} - 1) = E(\overline{X}) - 1 = E(X) - 1 = (1 + \theta) - 1 = \theta$$

Luego, el estimador es insesgado y por lo tanto, asintóticamente insesgado.

## c) Varianza

Calculamos la varianza del estimador.

$$Var(\hat\theta) = Var(\overline{X} - 1) = Var(\overline{X}) = \frac{Var(X)}{n}$$

Para calcular $Var(X)$ necesitamos primero $E(X^2)$. Usando el cambio de variable $u = x - \theta$:

$$E(X^2) = \int_{\theta}^{\infty} x^2 e^{-(x-\theta)} dx = \int_{0}^{\infty} (u + \theta)^2 e^{-u} du$$

$$= \int_{0}^{\infty} (u^2 + 2\theta u + \theta^2) e^{-u} du$$

$$= \int_{0}^{\infty} u^2 e^{-u} du + 2\theta \int_{0}^{\infty} u e^{-u} du + \theta^2 \int_{0}^{\infty} e^{-u} du$$

$$= \Gamma(3) + 2\theta \cdot \Gamma(2) + \theta^2 \cdot 1 = 2 + 2\theta + \theta^2$$

Ahora calculamos la varianza de $X$:

$$Var(X) = E(X^2) - [E(X)]^2 = (2 + 2\theta + \theta^2) - (1 + \theta)^2$$

$$= 2 + 2\theta + \theta^2 - (1 + 2\theta + \theta^2) = 1$$

Por lo tanto:

$$Var(\hat\theta) = \frac{1}{n}$$

## d) ECM

Calculamos el error cuadrático medio (ECM).

Dado que el estimador es insesgado, el ECM es igual a la varianza:

$$ECM(\hat\theta) = Var(\hat\theta) = \frac{1}{n}$$

## e) Simulación

Elegimos el valor $\theta=2$ y simulamos una muestra aleatoria de tamaño $n=1000$. Calculamos el estimador y comparamos el histograma de los valores simulados con el valor real del parámetro y la función de densidad obtenida con el valor estimado del parámetro.

Para generar la muestra aleatoria, necesitamos encontrar la función de distribución acumulada y su inversa.

La función de distribución acumulada es:

$$F_X(x;\theta) = \int_{\theta}^{x} e^{-(t-\theta)} dt$$

Usando $u = t - \theta$:

$$F_X(x;\theta) = \int_{0}^{x-\theta} e^{-u} du = [-e^{-u}]_{0}^{x-\theta} = 1 - e^{-(x-\theta)}, \quad x \geq \theta$$

Para encontrar la inversa, igualamos $U = F_X(x;\theta)$ donde $U \sim \text{Unif}(0,1)$:

$$U = 1 - e^{-(x-\theta)} \implies e^{-(x-\theta)} = 1 - U$$

$$-(x-\theta) = \ln(1-U) \implies x = \theta - \ln(1-U)$$

Como $1-U$ también tiene distribución $\text{Unif}(0,1)$, podemos usar:

$$X = \theta - \ln(U)$$

```{r}
# Parámetro fijo
theta_fijo <- 2

# Función de densidad
dexr <- function(x, theta){
  f_x <- ifelse(x >= theta, exp(-(x - theta)), 0)
  return(f_x)
}

# Función para generar muestra aleatoria
rexr <- function(n, theta){
  U <- runif(n)
  X <- theta - log(U)
  return(X)
}

# Función para estimar theta
estimar_theta <- function(X){
  theta_hat <- mean(X) - 1
  return(theta_hat)
}

# Generar muestra
df_exr <- data.frame(X = rexr(5000, theta_fijo))
theta_hat <- estimar_theta(df_exr$X)

# Gráfico
library(ggplot2)

ggplot(df_exr) +
  geom_histogram(aes(x = X, y = after_stat(density)), 
                 binwidth = 0.3, color = "black", fill = "coral3", 
                 alpha = 0.7, boundary = theta_fijo) +
  stat_function(fun = dexr, args = list(theta = theta_hat), 
                color = "blue", linewidth = 1, 
                xlim = c(theta_hat, max(df_exr$X))) +
  annotate("text", x = theta_fijo + 3, y = 0.7, 
           label = paste("θ̂ =", round(theta_hat, 3)), 
           color = "blue", size = 5) +
  labs(title = "Histograma de datos y función de densidad estimada",
       x = "Valores",
       y = "Densidad") +
  theme_minimal()
```


**Resultado:** Con $\theta = 2$, el valor estimado es $\hat\theta \approx 2.00$ (varía en cada simulación).

## f) Convergencia

Verificamos la convergencia del estimador al aumentar el tamaño cada muestra. Graficamos los valores del estimador en función del tamaño de la muestra con $n = 10, 50, 100, 500, 1000$. Para cada $n$ se generan $N = 500$ valores.

```{r}
# Tamaños de muestra y número de réplicas
tamano <- c(10, 50, 100, 500, 1000)
N <- 500
# Data frame para almacenar los resultados
df_convergencia <- data.frame()

# Simulación y cálculo del estimador para cada tamaño de muestra
for (n in tamano){
  estimacion_n <- replicate(N, {
    X <- rexr(n, theta_fijo)
    estimar_theta(X)
  })
  df_temp <- data.frame(n = factor(n), Estimacion = estimacion_n)
  df_convergencia <- rbind(df_convergencia, df_temp)
}

# Gráfico de convergencia
ggplot(df_convergencia) +
  geom_boxplot(aes(x = n, y = Estimacion, fill = n), alpha = 0.7) +
  geom_hline(yintercept = theta_fijo, color = "red", 
             linetype = "dashed", linewidth = 1) +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Convergencia del estimador al aumentar el tamaño de la muestra",
       x = "Tamaño de la muestra (n)",
       y = "Estimación de θ") +
  theme_bw() +
  theme(legend.position = "none")
```


El gráfico muestra que a medida que aumenta el tamaño de la muestra, las estimaciones se concentran cada vez más alrededor del valor real $\theta = 2$ (línea roja punteada), confirmando la **consistencia** del estimador. Además, observamos que $Var(\hat\theta) = 1/n$, lo que implica que la varianza disminuye inversamente con el tamaño de muestra.
:::
::::

::: {#exr-continua_2}
Para $\theta >0$, consideramos la función de densidad:

\begin{equation}
f(x;\theta) = \begin{cases}
\theta x^{\theta-1} & \text{si } 0< x < 1 \\
0 & \text{en otro caso.}
\end{cases}
\end{equation}
:::

::: panel-tabset

## a) Estimador

Encontramos el estimador para $\theta$ por el método de momentos.

Primero calculamos la esperanza de la variable aleatoria $X$:

$$E(X) = \int_{-\infty}^{\infty} x f(x;\theta) dx = \int_{0}^{1} x \cdot \theta x^{\theta-1} dx$$

$$= \theta \int_{0}^{1} x^{\theta} dx = \theta \left[\frac{x^{\theta+1}}{\theta+1}\right]_{0}^{1}$$

$$= \theta \cdot \frac{1}{\theta+1} = \frac{\theta}{\theta+1}$$

Igualamos con la media muestral:

$$\overline{X} = \frac{\hat\theta}{\hat\theta+1}$$

Despejamos $\hat\theta$:

$$\overline{X}(\hat\theta+1) = \hat\theta$$

$$\overline{X}\hat\theta + \overline{X} = \hat\theta$$

$$\overline{X} = \hat\theta - \overline{X}\hat\theta$$

$$\overline{X} = \hat\theta(1 - \overline{X})$$

$$\hat\theta = \frac{\overline{X}}{1 - \overline{X}}$$

## b) Insesgamiento

Verificamos si es insesgado y/o asintóticamente insesgado.

Para esto necesitamos calcular $E(\hat\theta) = E\left(\frac{\overline{X}}{1 - \overline{X}}\right)$.

Esta esperanza no es sencilla de calcular directamente porque $\hat\theta$ es una función no lineal de $\overline{X}$. Usaremos la aproximación de Taylor para analizar el sesgo asintótico.

Sea $g(x) = \frac{x}{1-x}$. Expandiendo alrededor de $\mu = E(\overline{X}) = E(X) = \frac{\theta}{\theta+1}$:

$$g(\overline{X}) \approx g(\mu) + g'(\mu)(\overline{X} - \mu) + \frac{g''(\mu)}{2}(\overline{X} - \mu)^2$$

Calculamos las derivadas:

$$g'(x) = \frac{(1-x) + x}{(1-x)^2} = \frac{1}{(1-x)^2}$$

$$g''(x) = \frac{2}{(1-x)^3}$$

Evaluando en $\mu = \frac{\theta}{\theta+1}$:

$$g(\mu) = \frac{\frac{\theta}{\theta+1}}{1-\frac{\theta}{\theta+1}} = \frac{\frac{\theta}{\theta+1}}{\frac{1}{\theta+1}} = \theta$$

$$g'(\mu) = \frac{1}{\left(\frac{1}{\theta+1}\right)^2} = (\theta+1)^2$$

Por lo tanto:

$$E(\hat\theta) \approx \theta + \frac{g''(\mu)}{2}Var(\overline{X}) + \text{términos de orden superior}$$

Como $Var(\overline{X}) = \frac{Var(X)}{n} \to 0$ cuando $n \to \infty$, tenemos:

$$\lim_{n \to \infty} E(\hat\theta) = \theta$$

El estimador **NO es insesgado** para muestras finitas, pero **SÍ es asintóticamente insesgado**.

## c) Varianza

Calculamos la varianza del estimador usando el método delta.

Para $g(x) = \frac{x}{1-x}$, tenemos $g'(x) = \frac{1}{(1-x)^2}$.

Por el método delta:

$$Var(\hat\theta) \approx [g'(E(\overline{X}))]^2 Var(\overline{X}) = [g'(\mu)]^2 \frac{Var(X)}{n}$$

donde $\mu = \frac{\theta}{\theta+1}$ y $g'(\mu) = (\theta+1)^2$.

Necesitamos calcular $Var(X)$. Primero calculamos $E(X^2)$:

$$E(X^2) = \int_{0}^{1} x^2 \cdot \theta x^{\theta-1} dx = \theta \int_{0}^{1} x^{\theta+1} dx$$

$$= \theta \left[\frac{x^{\theta+2}}{\theta+2}\right]_{0}^{1} = \frac{\theta}{\theta+2}$$

Entonces:

$$Var(X) = E(X^2) - [E(X)]^2 = \frac{\theta}{\theta+2} - \left(\frac{\theta}{\theta+1}\right)^2$$

$$= \frac{\theta}{\theta+2} - \frac{\theta^2}{(\theta+1)^2} = \frac{\theta(\theta+1)^2 - \theta^2(\theta+2)}{(\theta+2)(\theta+1)^2}$$

$$= \frac{\theta[(\theta+1)^2 - \theta(\theta+2)]}{(\theta+2)(\theta+1)^2} = \frac{\theta[\theta^2+2\theta+1 - \theta^2-2\theta]}{(\theta+2)(\theta+1)^2}$$

$$= \frac{\theta}{(\theta+2)(\theta+1)^2}$$

Por lo tanto:

$$Var(\hat\theta) \approx (\theta+1)^4 \cdot \frac{1}{n} \cdot \frac{\theta}{(\theta+2)(\theta+1)^2}$$

$$= \frac{\theta(\theta+1)^2}{n(\theta+2)}$$

## d) ECM

Calculamos el error cuadrático medio (ECM).

$$ECM(\hat\theta) = Var(\hat\theta) + [Sesgo(\hat\theta)]^2$$

Para muestras grandes, el sesgo es aproximadamente cero, por lo que:

$$ECM(\hat\theta) \approx \frac{\theta(\theta+1)^2}{n(\theta+2)}$$

## e) Simulación

Elegimos el valor $\theta=3$ y simulamos una muestra aleatoria de tamaño $n=1000$. Calculamos el estimador y comparamos el histograma de los valores simulados con el valor real del parámetro y la función de densidad obtenida con el valor estimado del parámetro.

Para generar la muestra aleatoria, necesitamos encontrar la función de distribución acumulada y su inversa.

La función de distribución acumulada es:

$$F_X(x;\theta) = \int_{0}^{x} \theta t^{\theta-1} dt = \theta \left[\frac{t^{\theta}}{\theta}\right]_{0}^{x} = x^{\theta}, \quad 0 < x < 1$$

Para encontrar la inversa, igualamos $U = F_X(x;\theta)$ donde $U \sim \text{Unif}(0,1)$:

$$U = x^{\theta} \implies x = U^{1/\theta}$$

```{r}
# Parámetro fijo
theta_fijo <- 3

# Función de densidad
dexr <- function(x, theta){
  f_x <- ifelse(x > 0 & x < 1, theta * x^(theta - 1), 0)
  return(f_x)
}

# Función para generar muestra aleatoria
rexr <- function(n, theta){
  U <- runif(n)
  X <- U^(1/theta)
  return(X)
}

# Función para estimar theta
estimar_theta <- function(X){
  theta_hat <- mean(X) / (1 - mean(X))
  return(theta_hat)
}

# Generar muestra
df_exr <- data.frame(X = rexr(5000, theta_fijo))
theta_hat <- estimar_theta(df_exr$X)

# Gráfico
library(ggplot2)

ggplot(df_exr) +
  geom_histogram(aes(x = X, y = after_stat(density)), 
                 binwidth = 0.03, color = "black", fill = "coral3", 
                 alpha = 0.7, boundary = 0) +
  stat_function(fun = dexr, args = list(theta = theta_hat), 
                color = "blue", linewidth = 1, 
                xlim = c(0, 1)) +
  annotate("text", x = 0.5, y = 2, 
           label = paste("θ̂ =", round(theta_hat, 3)), 
           color = "blue", size = 5) +
  labs(title = "Histograma de datos y función de densidad estimada",
       x = "Valores",
       y = "Densidad") +
  theme_minimal()
```


**Resultado:** Con $\theta = 3$, el valor estimado es $\hat\theta \approx 3.00$ (varía en cada simulación).

## f) Convergencia

Verificamos la convergencia del estimador al aumentar el tamaño cada muestra. Graficamos los valores del estimador en función del tamaño de la muestra con $n = 10, 50, 100, 500, 1000$. Para cada $n$ se generan $N = 500$ valores.

```{r}
# Tamaños de muestra y número de réplicas
tamano <- c(10, 50, 100, 500, 1000)
N <- 500

# Data frame para almacenar los resultados
df_convergencia <- data.frame()

# Simulación y cálculo del estimador para cada tamaño de muestra
for (n in tamano){
  estimacion_n <- replicate(N, {
    X <- rexr(n, theta_fijo)
    estimar_theta(X)
  })
  df_temp <- data.frame(n = factor(n), Estimacion = estimacion_n)
  df_convergencia <- rbind(df_convergencia, df_temp)
}

# Gráfico de convergencia
library(RColorBrewer)

ggplot(df_convergencia) +
  geom_boxplot(aes(x = n, y = Estimacion, fill = n), alpha = 0.7) +
  geom_hline(yintercept = theta_fijo, color = "red", 
             linetype = "dashed", linewidth = 1) +
  scale_fill_brewer(palette = "Set1") +
  labs(title = "Convergencia del estimador al aumentar el tamaño de la muestra",
       x = "Tamaño de la muestra (n)",
       y = "Estimación de θ") +
  theme_bw() +
  theme(legend.position = "none")
```


El gráfico muestra que a medida que aumenta el tamaño de la muestra, las estimaciones se concentran cada vez más alrededor del valor real $\theta = 3$ (línea roja punteada), confirmando la **consistencia** del estimador. Aunque el estimador tiene un pequeño sesgo para muestras finitas, este sesgo desaparece asintóticamente.
:::
::::
